{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognizer (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is one of the very useful information extraction technique to identify and classify \n",
    "named entities in text. These entities are pre-defined categories such a personâ€™s names, organizations, \n",
    "locations, time representations, financial elements, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('words')\n",
    "#nltk.download('abc')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag,ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''Andrew Yan-Tak Ng is a Chinese American computer scientist.\n",
    "He is the former chief scientist at Baidu, where he led the company's\n",
    "Artificial Intelligence Group. He is an adjunct professor (formerly \n",
    "associate professor) at Stanford University. Ng is also the co-founder\n",
    "and chairman at Coursera, an online education platform. Andrew was born\n",
    "in the UK in 1976. His parents were both from Hong Kong.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Andrew', 'PERSON'), ('Chinese', 'GPE'), ('American', 'GPE'), ('Baidu', 'ORGANIZATION'), (\"company's Artificial Intelligence Group\", 'ORGANIZATION'), ('Stanford University', 'ORGANIZATION'), ('Coursera', 'ORGANIZATION'), ('Andrew', 'PERSON'), ('Hong Kong', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "def nltk_ner(document):\n",
    "    '''\n",
    "    fetch the Named entity from the document using NLTK\n",
    "    '''\n",
    "    ##tokenize the doc\n",
    "    tokenized_doc = word_tokenize(document)\n",
    "\n",
    "    ##tag the document \n",
    "    tag_doc = pos_tag(tokenized_doc)\n",
    "    tag_chunk_doc = ne_chunk(tag_doc)\n",
    "    ##extract the named entities\n",
    "    \n",
    "    named_entities=[]\n",
    "    for items in tag_chunk_doc:\n",
    "        #print('*',items)\n",
    "        if hasattr(items, 'label'):\n",
    "            ##named_entities=[]\n",
    "            ## c[0] is the word like Andrew c[1] contain Part of Speech\n",
    "            entity_name = ' '.join(c[0] for c in items.leaves())\n",
    "            entity_type = items.label()\n",
    "            named_entities.append((entity_name,entity_type))\n",
    "    return named_entities\n",
    "        \n",
    "named_entities= nltk_ner(document)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classical approaches: Rule Based NER( using Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requires eGenix.com mx Base Distribution\n",
      "http://www.egenix.com/products/python/mxBase/\n",
      "PM denies knowledge of AWB kickbacks\n",
      "The Prime Minister has denied he knew AWB was paying kickbacks to Iraq despite writing to the wheat exporter asking to be kept fully informed on Iraq wheat sales.\n",
      "Letters from John Howard and Deputy Prime Minister Mark Vaile to AWB have been released by the Cole inquiry into the oil for food program.\n",
      "In one of the letters Mr Howard asks AWB managing director Andrew Lindberg to remain in close contact with the Government on Iraq wheat sales.\n",
      "The Opposition's Gavan O'Connor says the letter was sent in <TIMEX2>2002</TIMEX2>, the same time AWB was paying kickbacks to Iraq though a Jordanian trucking company.\n",
      "He says the Government can longer wipe its hands of the illicit payments, which totalled $290 million.\n",
      "\"The responsibility for this must lay may squarely at the feet of Coalition ministers in trade, agriculture and the Prime Minister,\" he said.\n",
      "But the Prime Minister says letters show he was inquiring about the future of wheat sales in Iraq and do not prove the Government knew of the payments.\n",
      "\"It would have been astonishing in <TIMEX2>2002</TIMEX2> if as Prime Minister I hadn't done anything I possibly could to preserve Australia's very valuable wheat market,\" he said.\n",
      "\n",
      "Email questions\n",
      "<TIMEX2>Today</TIMEX2> at the inquiry, AWB trading manager Peter Geary has been questioned about an email he received in May <TIMEX2>2000</TIMEX2>.\n",
      "It indicated that the Iraqi Grains Board had approached AWB to provide \"after-sales service\".\n",
      "Mr Geary said he had forwarded the email to two AWB colleagues and did not remember reading it, although he said he may have skimmed it.\n",
      "\n",
      "Support\n",
      "AWB still has plenty of support among grain growers in central western New South Wales despite the revelations of the Cole inquiry.\n",
      "Producers say they broadly support AWB's attempts to get the best prices for their products.\n",
      "\"I think it's all a ploy by overseas interests to try and get the single desk put aside. The stories that are going round about the commission and everything, I think that's the way people have got to do things to do business with the Middle East and Asian countries,\" one producer said.\n",
      "\"I think it's actually a pretty reasonable system and I think actually I'd give them pretty fair support at the moment. I think on average they've performed fairly well,\" another producer said.\n",
      "\"The biggest thing about someone else taking over is whether the multinationals will get too much of a foothold in there and take it too much to their advantage.\"\n",
      "\n",
      "Grain prices\n",
      "But an analyst predicts grain prices will drop another $20 a tonne on the back of the inquiry into AWB.\n",
      "Malcolm Bartholomaeus says pool returns have already dropped by $20 a tonne <TIMEX2>this year</TIMEX2> from the average price over the past five years.\n",
      "He says the premiums that AWB was achieving through its wheat export monopoly have been severely eroded.\n",
      "\n",
      "SA farmers help fire ravaged neighbours\n",
      "Farmers in South Australia's south-east are donating truckloads of hay to their neighbours across the border in the wake of the Grampians bushfires.\n",
      "In just a few days, farmers have donated 250 tonnes of hay, as well as agistment for cattle.\n",
      "They say that is just the beginning.\n",
      "Fodder drive coordinator Peter O'Conner says he has been overwhelmed by the response.\n",
      "\"All the hay that's going <TIMEX2>this week</TIMEX2> has all gone from places that have donated one load or up to two loads of hay,\" he said.\n",
      "\"We've got one man that's donated two full loads and the rest are all one loads, straight loads that we're moving <TIMEX2>this week</TIMEX2>.\n",
      "\n",
      "Floodwaters close highway\n",
      "A major highway between the Northern Territory and Western Australia remains blocked by floodwaters <TIMEX2>today</TIMEX2>.\n",
      "The Victoria River has cut the Victoria Highway and also flooded the remote Pigeon Hole Aboriginal community.\n",
      "Stockman Simon Ham describes the relocation of a hundred people to higher ground.\n",
      "\"They had all their vehicles moved out of the community and they had a few camps set up and they were moving more people and belongings out with a couple of boats, just onto higher ground, only 500 metres from the community,\" he said.\n",
      "\"They are up on a sandstone ridge at the moment and I think the river heights might be up for a little longer but I think it [the Victoria River] will start going down.\"\n",
      "\n",
      "Grain company sold for $9m\n",
      "Tasmania's main grain company has changed hands for the second time in just three years.\n",
      "The former state-owned Tasmanian Grain Elevators Board has been sold to local agribusiness Roberts Limited for about $9 million.\n",
      "The deal includes silos at Launceston, Devonport and Powranna in northern Tasmania.\n",
      "John Macleod from Roberts Limited says the company bid when the board was first privatised.\n",
      "\"We were very disappointed we weren't successful at that point of time,\" he said.\n",
      "\n",
      "Wine workers stop work\n",
      "Workers at Hardy Wines' Stanley winery in south-western New South Wales have walked off the job for the second time in a week.\n",
      "Forty staff walked out this morning in a dispute over a new enterprise bargaining agreement.\n",
      "The stopwork comes just as the region's wine grape crush gets under way.\n",
      "Hardy Wines took the matter to the Industrial Relations Commission (IRC) on Friday.\n",
      "\n",
      "Wool body eyes sportswear industry\n",
      "The $50 billion global sportswear industry is the new target of wool promotion body, Australian Wool Innovation (AWI).\n",
      "AWI is showing wool blend t-shirts and casual wear to manufacturers <TIMEX2>this week</TIMEX2> at the largest trade show for the industry being held in Germany.\n",
      "AWI's Len Stephens says although wool-blend sportswear will be at the pricier end of the market, shoppers are willing to pay more.\n",
      "\"The sports apparel market sector is one of the biggest and certainly the fastest growing apparel sector in the world, and it's no secret that wool hasn't had a very big share of that market at all,\" he said.\n",
      "\"The level of wool, particularly Australian merino wool in the sports market is really almost below the radar.\n",
      "\n",
      "Organisation to step up banning rodeos campaign\n",
      "Tasmania's rodeo industry is under sustained attack from animal rights activists after another animal had to be destroyed at a weekend event.\n",
      "A bucking horse was put down after breaking a leg at Ulverstone on Saturday.\n",
      "<TIMEX2>Two weeks ago</TIMEX2>, a bull was destroyed after apparently breaking its back during a bull-riding competition at Carrick.\n",
      "The owner of both animals, Brian Fish, says they were unfortunate accidents and not an animal welfare issue.\n",
      "But Emma Haswell from Against Animal Cruelty Tasmania says her organisation will up its efforts to have rodeos banned.\n",
      "\n",
      "Snails used in cancer research\n",
      "New research is under way to investigate whether South Australian sea snails could eventually be used to treat cancer.\n",
      "Until now the state's snail population has been virtually unexplored but Flinders University hopes to discover beneficial compounds in predatory sea snails, known as whelks.\n",
      "Marine biologist Dr Kirsten Benkendorff says the work follows research overseas, where clinical trials are now investigating anti-cancer properties.\n",
      "\"The whelks that I'm looking at at the moment are not currently harvested and not really considered a useful resource so I think that there's a lot of potential economic benefits from that side of things,\" she said.\n",
      "\n",
      "Processors fail to meet kangaroo meat demand\n",
      "International demand for kangaroo meat is putting pressure on Australian processors.\n",
      "Despite high prices, harvesting quotas are not being met leaving a big gap in the market.\n",
      "Phil Franolick from King River International Processors in Perth says Europeans are attracted to the meat's low fat content and Russians are consuming large amounts of kangaroo sausages and mince.\n",
      "\"As far as I know every skerrick of manufactured meat that is produced by human consumption processors virtually goes to the Russian market now,\" he said.\n",
      "\n",
      "People warned to be on snake alert\n",
      "With temperatures soaring this summer, particularly in southern Australia, snakes are on the move.\n",
      "People are being warned to keep their eyes open out in the paddock, especially in areas hit by bushfires, with snakes forced from their usual habitat.\n",
      "Maria Dovey from Reptile Rescue Tasmania has these words of advice.\n",
      "\"Any people who are regularly outdoors, whether they be camping, bush walking, people who're on the land working, they must always carry at least two bandages with them and a mobile phone,\" she said.\n",
      "\n",
      "Aerial spraying begins to control locust threat\n",
      "Victoria's high country farmers have begun aerial spraying to try to control the threat of locusts.\n",
      "It is the first time locust plagues have made it to the region, with continued rain and warm weather assisting breeding conditions.\n",
      "Pastures and crops have already been wiped out by the pest.\n",
      "\n",
      "Injury won't stop baton runner\n",
      "A southern Queensland cattle farmer says a knee injury will not stop him running in the Queen's baton relay <TIMEX2>today</TIMEX2> in the lead-up to the Commonwealth Games.\n",
      "Stanthorpe's Bill Bonner travelled to Coffs Harbour in New South Wales to run his leg of the relay.\n",
      "And he says injury will not prevent him from making the 500 metre dash.\n",
      "\"About <TIMEX2>two weeks ago</TIMEX2> I was jumping off my motorbike and twisted a ligament in my knee and I haven't been able to do more than walk since,\" he said.\n",
      "\"But I promise you I will run that baton because you get such an adrenalin rush that I think you could only have one leg and you could still run it.\"\n",
      "\n",
      "The NZ apple debate continues\n",
      "In a New Zealand newspaper on the weekend, members of the New Zealand apple industry voiced concern regarding the legitimacy of the \"science\" used in the draft Import Risk Analysis put out <TIMEX2>last month</TIMEX2> by Biosecurity Australia. The draft Import Risk Analysis (IRA) determines the risk of fireblight and other pests and diseases if New Zealand apples are imported into Australia.\n",
      "Under the World Trade Organisation's rules Australia can refuse the importation of New Zealand apples if they are able to provide scientific evidence of a substantial disease or pest risk. But according to the New Zealand apple growers, \"professional\" scientific evidence shows that mature apples imported into Australia pose no ri\n"
     ]
    }
   ],
   "source": [
    "# Code for tagging temporal expressions in text\n",
    "# For details of the TIMEX format, see http://timex2.mitre.org/\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Requires eGenix.com mx Base Distribution\n",
    "# http://www.egenix.com/products/python/mxBase/\n",
    "try:\n",
    "    from mx.DateTime import *\n",
    "except ImportError:\n",
    "    print(\"\"\"\n",
    "Requires eGenix.com mx Base Distribution\n",
    "http://www.egenix.com/products/python/mxBase/\"\"\")\n",
    "\n",
    "# Predefined strings.\n",
    "numbers = \"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten| \\\n",
    "          eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen| \\\n",
    "          eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty| \\\n",
    "          ninety|hundred|thousand)\"\n",
    "day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
    "week_day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
    "month = \"(january|february|march|april|may|june|july|august|september| \\\n",
    "          october|november|december)\"\n",
    "dmy = \"(year|day|week|month)\"\n",
    "rel_day = \"(today|yesterday|tomorrow|tonight|tonite)\"\n",
    "exp1 = \"(before|after|earlier|later|ago)\"\n",
    "exp2 = \"(this|next|last)\"\n",
    "iso = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n",
    "year = \"((?<=\\s)\\d{4}|^\\d{4})\"\n",
    "regxp1 = \"((\\d+|(\" + numbers + \"[-\\s]?)+) \" + dmy + \"s? \" + exp1 + \")\"\n",
    "regxp2 = \"(\" + exp2 + \" (\" + dmy + \"|\" + week_day + \"|\" + month + \"))\"\n",
    "\n",
    "reg1 = re.compile(regxp1, re.IGNORECASE)\n",
    "reg2 = re.compile(regxp2, re.IGNORECASE)\n",
    "reg3 = re.compile(rel_day, re.IGNORECASE)\n",
    "reg4 = re.compile(iso)\n",
    "reg5 = re.compile(year)\n",
    "\n",
    "def tag(text):\n",
    "\n",
    "    # Initialization\n",
    "    timex_found = []\n",
    "\n",
    "    # re.findall() finds all the substring matches, keep only the full\n",
    "    # matching string. Captures expressions such as 'number of days' ago, etc.\n",
    "    found = reg1.findall(text)\n",
    "    found = [a[0] for a in found if len(a) > 1]\n",
    "    for timex in found:\n",
    "        timex_found.append(timex)\n",
    "\n",
    "    # Variations of this thursday, next year, etc\n",
    "    found = reg2.findall(text)\n",
    "    found = [a[0] for a in found if len(a) > 1]\n",
    "    for timex in found:\n",
    "        timex_found.append(timex)\n",
    "\n",
    "    # today, tomorrow, etc\n",
    "    found = reg3.findall(text)\n",
    "    for timex in found:\n",
    "        timex_found.append(timex)\n",
    "\n",
    "    # ISO\n",
    "    found = reg4.findall(text)\n",
    "    for timex in found:\n",
    "        timex_found.append(timex)\n",
    "\n",
    "    # Year\n",
    "    found = reg5.findall(text)\n",
    "    for timex in found:\n",
    "        timex_found.append(timex)\n",
    "\n",
    "    # Tag only temporal expressions which haven't been tagged.\n",
    "    for timex in timex_found:\n",
    "        text = re.sub(timex + '(?!</TIMEX2>)', '<TIMEX2>' + timex + '</TIMEX2>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Hash function for week days to simplify the grounding task.\n",
    "# [Mon..Sun] -> [0..6]\n",
    "hashweekdays = {\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6}\n",
    "\n",
    "# Hash function for months to simplify the grounding task.\n",
    "# [Jan..Dec] -> [1..12]\n",
    "hashmonths = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12}\n",
    "\n",
    "# Hash number in words into the corresponding integer value\n",
    "def hashnum(number):\n",
    "    if re.match(r'one|^a\\b', number, re.IGNORECASE):\n",
    "        return 1\n",
    "    if re.match(r'two', number, re.IGNORECASE):\n",
    "        return 2\n",
    "    if re.match(r'three', number, re.IGNORECASE):\n",
    "        return 3\n",
    "    if re.match(r'four', number, re.IGNORECASE):\n",
    "        return 4\n",
    "    if re.match(r'five', number, re.IGNORECASE):\n",
    "        return 5\n",
    "    if re.match(r'six', number, re.IGNORECASE):\n",
    "        return 6\n",
    "    if re.match(r'seven', number, re.IGNORECASE):\n",
    "        return 7\n",
    "    if re.match(r'eight', number, re.IGNORECASE):\n",
    "        return 8\n",
    "    if re.match(r'nine', number, re.IGNORECASE):\n",
    "        return 9\n",
    "    if re.match(r'ten', number, re.IGNORECASE):\n",
    "        return 10\n",
    "    if re.match(r'eleven', number, re.IGNORECASE):\n",
    "        return 11\n",
    "    if re.match(r'twelve', number, re.IGNORECASE):\n",
    "        return 12\n",
    "    if re.match(r'thirteen', number, re.IGNORECASE):\n",
    "        return 13\n",
    "    if re.match(r'fourteen', number, re.IGNORECASE):\n",
    "        return 14\n",
    "    if re.match(r'fifteen', number, re.IGNORECASE):\n",
    "        return 15\n",
    "    if re.match(r'sixteen', number, re.IGNORECASE):\n",
    "        return 16\n",
    "    if re.match(r'seventeen', number, re.IGNORECASE):\n",
    "        return 17\n",
    "    if re.match(r'eighteen', number, re.IGNORECASE):\n",
    "        return 18\n",
    "    if re.match(r'nineteen', number, re.IGNORECASE):\n",
    "        return 19\n",
    "    if re.match(r'twenty', number, re.IGNORECASE):\n",
    "        return 20\n",
    "    if re.match(r'thirty', number, re.IGNORECASE):\n",
    "        return 30\n",
    "    if re.match(r'forty', number, re.IGNORECASE):\n",
    "        return 40\n",
    "    if re.match(r'fifty', number, re.IGNORECASE):\n",
    "        return 50\n",
    "    if re.match(r'sixty', number, re.IGNORECASE):\n",
    "        return 60\n",
    "    if re.match(r'seventy', number, re.IGNORECASE):\n",
    "        return 70\n",
    "    if re.match(r'eighty', number, re.IGNORECASE):\n",
    "        return 80\n",
    "    if re.match(r'ninety', number, re.IGNORECASE):\n",
    "        return 90\n",
    "    if re.match(r'hundred', number, re.IGNORECASE):\n",
    "        return 100\n",
    "    if re.match(r'thousand', number, re.IGNORECASE):\n",
    "      return 1000\n",
    "\n",
    "# Given a timex_tagged_text and a Date object set to base_date,\n",
    "# returns timex_grounded_text\n",
    "def ground(tagged_text, base_date):\n",
    "\n",
    "    # Find all identified timex and put them into a list\n",
    "    timex_regex = re.compile(r'<TIMEX2>.*?</TIMEX2>', re.DOTALL)\n",
    "    timex_found = timex_regex.findall(tagged_text)\n",
    "    timex_found = map(lambda timex:re.sub(r'</?TIMEX2.*?>', '', timex), \\\n",
    "                timex_found)\n",
    "\n",
    "    # Calculate the new date accordingly\n",
    "    for timex in timex_found:\n",
    "        timex_val = 'UNKNOWN' # Default value\n",
    "\n",
    "        timex_ori = timex   # Backup original timex for later substitution\n",
    "\n",
    "        # If numbers are given in words, hash them into corresponding numbers.\n",
    "        # eg. twenty five days ago --> 25 days ago\n",
    "        if re.search(numbers, timex, re.IGNORECASE):\n",
    "            split_timex = re.split(r'\\s(?=days?|months?|years?|weeks?)', \\\n",
    "                                                              timex, re.IGNORECASE)\n",
    "            value = split_timex[0]\n",
    "            unit = split_timex[1]\n",
    "            num_list = map(lambda s:hashnum(s),re.findall(numbers + '+', \\\n",
    "                                          value, re.IGNORECASE))\n",
    "            timex = str(sum(num_list)) + ' ' + unit\n",
    "\n",
    "        # If timex matches ISO format, remove 'time' and reorder 'date'\n",
    "        if re.match(r'\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+', timex):\n",
    "            dmy = re.split(r'\\s', timex)[0]\n",
    "            dmy = re.split(r'/|-', dmy)\n",
    "            timex_val = str(dmy[2]) + '-' + str(dmy[1]) + '-' + str(dmy[0])\n",
    "\n",
    "        # Specific dates\n",
    "        elif re.match(r'\\d{4}', timex):\n",
    "            timex_val = str(timex)\n",
    "\n",
    "        # Relative dates\n",
    "        elif re.match(r'tonight|tonite|today', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date)\n",
    "        elif re.match(r'yesterday', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date + RelativeDateTime(days=-1))\n",
    "        elif re.match(r'tomorrow', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date + RelativeDateTime(days=+1))\n",
    "\n",
    "        # Weekday in the previous week.\n",
    "        elif re.match(r'last ' + week_day, timex, re.IGNORECASE):\n",
    "            day = hashweekdays[timex.split()[1]]\n",
    "            timex_val = str(base_date + RelativeDateTime(weeks=-1, \\\n",
    "                            weekday=(day,0)))\n",
    "\n",
    "        # Weekday in the current week.\n",
    "        elif re.match(r'this ' + week_day, timex, re.IGNORECASE):\n",
    "            day = hashweekdays[timex.split()[1]]\n",
    "            timex_val = str(base_date + RelativeDateTime(weeks=0, \\\n",
    "                            weekday=(day,0)))\n",
    "\n",
    "        # Weekday in the following week.\n",
    "        elif re.match(r'next ' + week_day, timex, re.IGNORECASE):\n",
    "            day = hashweekdays[timex.split()[1]]\n",
    "            timex_val = str(base_date + RelativeDateTime(weeks=+1, \\\n",
    "                              weekday=(day,0)))\n",
    "\n",
    "        # Last, this, next week.\n",
    "        elif re.match(r'last week', timex, re.IGNORECASE):\n",
    "            year = (base_date + RelativeDateTime(weeks=-1)).year\n",
    "\n",
    "            # iso_week returns a triple (year, week, day) hence, retrieve\n",
    "            # only week value.\n",
    "            week = (base_date + RelativeDateTime(weeks=-1)).iso_week[1]\n",
    "            timex_val = str(year) + 'W' + str(week)\n",
    "        elif re.match(r'this week', timex, re.IGNORECASE):\n",
    "            year = (base_date + RelativeDateTime(weeks=0)).year\n",
    "            week = (base_date + RelativeDateTime(weeks=0)).iso_week[1]\n",
    "            timex_val = str(year) + 'W' + str(week)\n",
    "        elif re.match(r'next week', timex, re.IGNORECASE):\n",
    "            year = (base_date + RelativeDateTime(weeks=+1)).year\n",
    "            week = (base_date + RelativeDateTime(weeks=+1)).iso_week[1]\n",
    "            timex_val = str(year) + 'W' + str(week)\n",
    "\n",
    "        # Month in the previous year.\n",
    "        elif re.match(r'last ' + month, timex, re.IGNORECASE):\n",
    "            month = hashmonths[timex.split()[1]]\n",
    "            timex_val = str(base_date.year - 1) + '-' + str(month)\n",
    "\n",
    "        # Month in the current year.\n",
    "        elif re.match(r'this ' + month, timex, re.IGNORECASE):\n",
    "            month = hashmonths[timex.split()[1]]\n",
    "            timex_val = str(base_date.year) + '-' + str(month)\n",
    "\n",
    "        # Month in the following year.\n",
    "        elif re.match(r'next ' + month, timex, re.IGNORECASE):\n",
    "            month = hashmonths[timex.split()[1]]\n",
    "            timex_val = str(base_date.year + 1) + '-' + str(month)\n",
    "        elif re.match(r'last month', timex, re.IGNORECASE):\n",
    "\n",
    "            # Handles the year boundary.\n",
    "            if base_date.month == 1:\n",
    "                timex_val = str(base_date.year - 1) + '-' + '12'\n",
    "            else:\n",
    "                timex_val = str(base_date.year) + '-' + str(base_date.month - 1)\n",
    "        elif re.match(r'this month', timex, re.IGNORECASE):\n",
    "                timex_val = str(base_date.year) + '-' + str(base_date.month)\n",
    "        elif re.match(r'next month', timex, re.IGNORECASE):\n",
    "\n",
    "            # Handles the year boundary.\n",
    "            if base_date.month == 12:\n",
    "                timex_val = str(base_date.year + 1) + '-' + '1'\n",
    "            else:\n",
    "                timex_val = str(base_date.year) + '-' + str(base_date.month + 1)\n",
    "        elif re.match(r'last year', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date.year - 1)\n",
    "        elif re.match(r'this year', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date.year)\n",
    "        elif re.match(r'next year', timex, re.IGNORECASE):\n",
    "            timex_val = str(base_date.year + 1)\n",
    "        elif re.match(r'\\d+ days? (ago|earlier|before)', timex, re.IGNORECASE):\n",
    "\n",
    "            # Calculate the offset by taking '\\d+' part from the timex.\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            timex_val = str(base_date + RelativeDateTime(days=-offset))\n",
    "        elif re.match(r'\\d+ days? (later|after)', timex, re.IGNORECASE):\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            timex_val = str(base_date + RelativeDateTime(days=+offset))\n",
    "        elif re.match(r'\\d+ weeks? (ago|earlier|before)', timex, re.IGNORECASE):\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            year = (base_date + RelativeDateTime(weeks=-offset)).year\n",
    "            week = (base_date + \\\n",
    "                            RelativeDateTime(weeks=-offset)).iso_week[1]\n",
    "            timex_val = str(year) + 'W' + str(week)\n",
    "        elif re.match(r'\\d+ weeks? (later|after)', timex, re.IGNORECASE):\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            year = (base_date + RelativeDateTime(weeks=+offset)).year\n",
    "            week = (base_date + RelativeDateTime(weeks=+offset)).iso_week[1]\n",
    "            timex_val = str(year) + 'W' + str(week)\n",
    "        elif re.match(r'\\d+ months? (ago|earlier|before)', timex, re.IGNORECASE):\n",
    "            extra = 0\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "\n",
    "            # Checks if subtracting the remainder of (offset / 12) to the base month\n",
    "            # crosses the year boundary.\n",
    "            if (base_date.month - offset % 12) < 1:\n",
    "                extra = 1\n",
    "\n",
    "            # Calculate new values for the year and the month.\n",
    "            year = str(base_date.year - offset // 12 - extra)\n",
    "            month = str((base_date.month - offset % 12) % 12)\n",
    "\n",
    "            # Fix for the special case.\n",
    "            if month == '0':\n",
    "                month = '12'\n",
    "            timex_val = year + '-' + month\n",
    "        elif re.match(r'\\d+ months? (later|after)', timex, re.IGNORECASE):\n",
    "            extra = 0\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            if (base_date.month + offset % 12) > 12:\n",
    "                extra = 1\n",
    "            year = str(base_date.year + offset // 12 + extra)\n",
    "            month = str((base_date.month + offset % 12) % 12)\n",
    "            if month == '0':\n",
    "                month = '12'\n",
    "            timex_val = year + '-' + month\n",
    "        elif re.match(r'\\d+ years? (ago|earlier|before)', timex, re.IGNORECASE):\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            timex_val = str(base_date.year - offset)\n",
    "        elif re.match(r'\\d+ years? (later|after)', timex, re.IGNORECASE):\n",
    "            offset = int(re.split(r'\\s', timex)[0])\n",
    "            timex_val = str(base_date.year + offset)\n",
    "\n",
    "        # Remove 'time' from timex_val.\n",
    "        # For example, If timex_val = 2000-02-20 12:23:34.45, then\n",
    "        # timex_val = 2000-02-20\n",
    "        timex_val = re.sub(r'\\s.*', '', timex_val)\n",
    "\n",
    "        # Substitute tag+timex in the text with grounded tag+timex.\n",
    "        tagged_text = re.sub('<TIMEX2>' + timex_ori + '</TIMEX2>', '<TIMEX2 val=\\\"' \\\n",
    "            + timex_val + '\\\">' + timex_ori + '</TIMEX2>', tagged_text)\n",
    "\n",
    "    return tagged_text\n",
    "\n",
    "####\n",
    "\n",
    "def demo(text):\n",
    "    '''\n",
    "    print the tagged text\n",
    "    '''\n",
    "    print(tag(text))\n",
    "\n",
    "## Pass the text to the function\n",
    "##if __name__ == '__main__':\n",
    "text = nltk.corpus.abc.raw('rural.txt')[:10000]\n",
    "demo(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Yan-Tak Ng is a Chinese American computer scientist.\n",
      "He is the former chief scientist at Baidu, where he led the company's\n",
      "Artificial Intelligence Group. He is an adjunct professor (formerly \n",
      "associate professor) at Stanford University. Ng is also the co-founder\n",
      "and chairman at Coursera, an online education platform. Andrew was born\n",
      "in the UK in <TIMEX2>1976</TIMEX2>. His parents were both from Hong Kong.\n"
     ]
    }
   ],
   "source": [
    "demo(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sent: ['Linux', 'is', 'the', 'best', 'OS']\n",
    "Labels: ['OS','IR','IR','IR','IR']\n",
    "Sent: ['Ubuntu', 'is', 'my', 'favorite', 'OS']\n",
    "Labels: ['OS','IR','IR','IR','IR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here the set of class labels are OS and IR\n",
    "## OS means Operating System\n",
    "## IR means Irrelevent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(['Linux', 'is', 'the', 'best', 'OS'], ['OS','IR','IR','IR','IR']),\n",
    "(['Ubuntu', 'is', 'my', 'favourite', 'OS'], ['OS','IR','IR','IR','IR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Linux', 'OS'), ('is', 'IR'), ('the', 'IR'), ('best', 'IR'), ('OS', 'IR')], [('Ubuntu', 'OS'), ('is', 'IR'), ('my', 'IR'), ('favourite', 'IR'), ('OS', 'IR')]]\n"
     ]
    }
   ],
   "source": [
    "def get_corpus(data):\n",
    "    '''\n",
    "    Get the word and tag to build the corpus\n",
    "    '''\n",
    "    corpus = []\n",
    "    for doc,tags in data:\n",
    "        doc_tag = []\n",
    "        for word, tag in zip(doc,tags):\n",
    "            doc_tag.append((word,tag))\n",
    "        corpus.append(doc_tag)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "corpus = get_corpus(data)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'current_word': 'Linux', 'BOS': True, 'next_word': 'is', '2nd_next_word': 'the'}, {'current_word': 'is', 'previous_word': 'Linux', 'next_word': 'the', '2nd_next_word': 'best'}, {'current_word': 'the', 'previous_word': 'is', '2nd_previous_word': 'Linux', 'next_word': 'best', '2nd_next_word': 'OS'}, {'current_word': 'best', 'previous_word': 'the', '2nd_previous_word': 'is', 'next_word': 'OS'}, {'current_word': 'OS', 'previous_word': 'best', '2nd_previous_word': 'the', 'EOS': True}], [{'current_word': 'Ubuntu', 'BOS': True, 'next_word': 'is', '2nd_next_word': 'my'}, {'current_word': 'is', 'previous_word': 'Ubuntu', 'next_word': 'my', '2nd_next_word': 'favourite'}, {'current_word': 'my', 'previous_word': 'is', '2nd_previous_word': 'Ubuntu', 'next_word': 'favourite', '2nd_next_word': 'OS'}, {'current_word': 'favourite', 'previous_word': 'my', '2nd_previous_word': 'is', 'next_word': 'OS'}, {'current_word': 'OS', 'previous_word': 'favourite', '2nd_previous_word': 'my', 'EOS': True}]]\n"
     ]
    }
   ],
   "source": [
    "def doc_2_features(doc,i):\n",
    "    '''\n",
    "    This function creates the features from the document\n",
    "    i here determines the position of the current word\n",
    "    doc is one documnet of the corpus\n",
    "    \n",
    "    This function return the dictionary of features for 1 document of the corpus\n",
    "    '''\n",
    "    features = dict()\n",
    "    ## Here each of the word of the doc is a tuple where the first one is the word \n",
    "    ## and the second one is the entity tag\n",
    "    features['current_word'] = doc[i][0]\n",
    "    if i >1:\n",
    "        features['previous_word'] = doc[i-1][0]\n",
    "        features['2nd_previous_word'] = doc[i-2][0]\n",
    "    elif i >0:\n",
    "        features['previous_word'] = doc[i-1][0]\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    ## -2 since max i value is len(doc) -1\n",
    "    if i< len(doc)-2:\n",
    "        features['next_word'] = doc[i+1][0]\n",
    "        features['2nd_next_word'] = doc[i+2][0]\n",
    "    elif i< len(doc)-1:\n",
    "        features['next_word'] = doc[i+1][0]\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(doc):\n",
    "    '''\n",
    "    Extract features from the doc\n",
    "    '''\n",
    "    return [doc_2_features(doc,i) for i in range(len(doc))]\n",
    "\n",
    "X = [extract_features(doc) for doc in corpus]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OS', 'IR', 'IR', 'IR', 'IR'], ['OS', 'IR', 'IR', 'IR', 'IR']]\n"
     ]
    }
   ],
   "source": [
    "def get_labels(doc):\n",
    "    '''\n",
    "    fetch the label of one document\n",
    "    '''\n",
    "    return [label for (word,label) in doc]\n",
    "\n",
    "y = [get_labels(doc) for doc in corpus]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "##%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OS', 'IR', 'IR', 'IR', 'IR', 'IR', 'IR']]\n",
      "********************************************\n",
      "['OS', 'IR', 'IR', 'IR', 'IR', 'IR', 'IR']\n"
     ]
    }
   ],
   "source": [
    "test = [['CentOS','happens' , 'to','be', 'my', 'favourite', 'OS']]\n",
    "X_test = [extract_features(doc) for doc in test]\n",
    "print(crf.predict(X_test))\n",
    "print('********************************************')\n",
    "## Here I am convert X_text from 2D to 1D for predict_single\n",
    "print(crf.predict_single(X_test[0])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spaCy for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"dataset/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
